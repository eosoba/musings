{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cardiff kaggle challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114321, 133)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>C</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.989780</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>AU</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636365</td>\n",
       "      <td>2.857144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598896</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.957825</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943877</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>C</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>...</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>2.477596</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>AE</td>\n",
       "      <td>1.773709</td>\n",
       "      <td>3.922193</td>\n",
       "      <td>1.120468</td>\n",
       "      <td>2</td>\n",
       "      <td>0.883118</td>\n",
       "      <td>1.176472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.797415</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>C</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>...</td>\n",
       "      <td>7.018256</td>\n",
       "      <td>1.812795</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>CJ</td>\n",
       "      <td>1.415230</td>\n",
       "      <td>2.954381</td>\n",
       "      <td>1.990847</td>\n",
       "      <td>1</td>\n",
       "      <td>1.677108</td>\n",
       "      <td>1.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1        v2 v3        v4         v5        v6        v7  \\\n",
       "0   3       1  1.335739  8.727474  C  3.921026   7.915266  2.599278  3.176895   \n",
       "1   4       1       NaN       NaN  C       NaN   9.191265       NaN       NaN   \n",
       "2   5       1  0.943877  5.310079  C  4.410969   5.326159  3.979592  3.928571   \n",
       "3   6       1  0.797415  8.304757  C  4.225930  11.627438  2.097700  1.987549   \n",
       "4   8       1       NaN       NaN  C       NaN        NaN       NaN       NaN   \n",
       "\n",
       "         v8    ...         v122      v123      v124  v125      v126      v127  \\\n",
       "0  0.012941    ...     8.000000  1.989780  0.035754    AU  1.804126  3.113719   \n",
       "1  2.301630    ...          NaN       NaN  0.598896    AF       NaN       NaN   \n",
       "2  0.019645    ...     9.333333  2.477596  0.013452    AE  1.773709  3.922193   \n",
       "3  0.171947    ...     7.018256  1.812795  0.002267    CJ  1.415230  2.954381   \n",
       "4       NaN    ...          NaN       NaN       NaN     Z       NaN       NaN   \n",
       "\n",
       "       v128  v129      v130      v131  \n",
       "0  2.024285     0  0.636365  2.857144  \n",
       "1  1.957825     0       NaN       NaN  \n",
       "2  1.120468     2  0.883118  1.176472  \n",
       "3  1.990847     1  1.677108  1.034483  \n",
       "4       NaN     0       NaN       NaN  \n",
       "\n",
       "[5 rows x 133 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "print train.shape\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114244"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['v125'].isnull().value_counts()[False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0.0\n",
      "*****************************\n",
      "target 0.0\n",
      "*****************************\n",
      "v1 43.589541729\n",
      "*****************************\n",
      "v2 43.5580514516\n",
      "*****************************\n",
      "v3 3.02394135811\n",
      "*****************************\n",
      "v4 43.5580514516\n",
      "*****************************\n",
      "v5 42.532867977\n",
      "*****************************\n",
      "v6 43.589541729\n",
      "*****************************\n",
      "v7 43.589541729\n",
      "*****************************\n",
      "v8 42.5284943274\n",
      "*****************************\n",
      "v9 43.6061615976\n",
      "*****************************\n",
      "v10 0.0734773138793\n",
      "*****************************\n",
      "v11 43.5930406487\n",
      "*****************************\n",
      "v12 0.0752267737336\n",
      "*****************************\n",
      "v13 43.589541729\n",
      "*****************************\n",
      "v14 0.00349891970854\n",
      "*****************************\n",
      "v15 43.5930406487\n",
      "*****************************\n",
      "v16 43.6446497144\n",
      "*****************************\n",
      "v17 43.5580514516\n",
      "*****************************\n",
      "v18 43.589541729\n",
      "*****************************\n",
      "v19 43.5991637582\n",
      "*****************************\n",
      "v20 43.5965395684\n",
      "*****************************\n",
      "v21 0.534459985479\n",
      "*****************************\n",
      "v22 0.437364963567\n",
      "*****************************\n",
      "v23 44.3269390576\n",
      "*****************************\n",
      "v24 0.0\n",
      "*****************************\n",
      "v25 42.5284943274\n",
      "*****************************\n",
      "v26 43.589541729\n",
      "*****************************\n",
      "v27 43.589541729\n",
      "*****************************\n",
      "v28 43.589541729\n",
      "*****************************\n",
      "v29 43.589541729\n",
      "*****************************\n",
      "v30 52.5800159201\n",
      "*****************************\n",
      "v31 3.02394135811\n",
      "*****************************\n",
      "v32 43.589541729\n",
      "*****************************\n",
      "v33 43.589541729\n",
      "*****************************\n",
      "v34 0.097095021912\n",
      "*****************************\n",
      "v35 43.589541729\n",
      "*****************************\n",
      "v36 42.532867977\n",
      "*****************************\n",
      "v37 43.5991637582\n",
      "*****************************\n",
      "v38 0.0\n",
      "*****************************\n",
      "v39 43.5930406487\n",
      "*****************************\n",
      "v40 0.097095021912\n",
      "*****************************\n",
      "v41 43.589541729\n",
      "*****************************\n",
      "v42 43.589541729\n",
      "*****************************\n",
      "v43 43.5930406487\n",
      "*****************************\n",
      "v44 43.5580514516\n",
      "*****************************\n",
      "v45 43.589541729\n",
      "*****************************\n",
      "v46 42.5284943274\n",
      "*****************************\n",
      "v47 0.0\n",
      "*****************************\n",
      "v48 43.5580514516\n",
      "*****************************\n",
      "v49 43.589541729\n",
      "*****************************\n",
      "v50 0.0752267737336\n",
      "*****************************\n",
      "v51 44.3295632473\n",
      "*****************************\n",
      "v52 0.0026241897814\n",
      "*****************************\n",
      "v53 43.5930406487\n",
      "*****************************\n",
      "v54 42.5284943274\n",
      "*****************************\n",
      "v55 43.589541729\n",
      "*****************************\n",
      "v56 6.01989135854\n",
      "*****************************\n",
      "v57 43.589541729\n",
      "*****************************\n",
      "v58 43.5930406487\n",
      "*****************************\n",
      "v59 43.5580514516\n",
      "*****************************\n",
      "v60 43.589541729\n",
      "*****************************\n",
      "v61 43.5580514516\n",
      "*****************************\n",
      "v62 0.0\n",
      "*****************************\n",
      "v63 42.5284943274\n",
      "*****************************\n",
      "v64 43.5580514516\n",
      "*****************************\n",
      "v65 43.5965395684\n",
      "*****************************\n",
      "v66 0.0\n",
      "*****************************\n",
      "v67 43.589541729\n",
      "*****************************\n",
      "v68 43.5930406487\n",
      "*****************************\n",
      "v69 43.6446497144\n",
      "*****************************\n",
      "v70 42.5433647361\n",
      "*****************************\n",
      "v71 0.0\n",
      "*****************************\n",
      "v72 0.0\n",
      "*****************************\n",
      "v73 43.5930406487\n",
      "*****************************\n",
      "v74 0.0\n",
      "*****************************\n",
      "v75 0.0\n",
      "*****************************\n",
      "v76 43.5580514516\n",
      "*****************************\n",
      "v77 43.589541729\n",
      "*****************************\n",
      "v78 43.6446497144\n",
      "*****************************\n",
      "v79 0.0\n",
      "*****************************\n",
      "v80 43.6061615976\n",
      "*****************************\n",
      "v81 42.532867977\n",
      "*****************************\n",
      "v82 42.532867977\n",
      "*****************************\n",
      "v83 43.589541729\n",
      "*****************************\n",
      "v84 43.589541729\n",
      "*****************************\n",
      "v85 44.3330621671\n",
      "*****************************\n",
      "v86 43.589541729\n",
      "*****************************\n",
      "v87 42.5669824442\n",
      "*****************************\n",
      "v88 43.589541729\n",
      "*****************************\n",
      "v89 42.5284943274\n",
      "*****************************\n",
      "v90 43.5930406487\n",
      "*****************************\n",
      "v91 0.0026241897814\n",
      "*****************************\n",
      "v92 43.5991637582\n",
      "*****************************\n",
      "v93 43.589541729\n",
      "*****************************\n",
      "v94 43.589541729\n",
      "*****************************\n",
      "v95 43.5991637582\n",
      "*****************************\n",
      "v96 43.589541729\n",
      "*****************************\n",
      "v97 43.5991637582\n",
      "*****************************\n",
      "v98 42.5591098748\n",
      "*****************************\n",
      "v99 43.589541729\n",
      "*****************************\n",
      "v100 43.5930406487\n",
      "*****************************\n",
      "v101 43.5580514516\n",
      "*****************************\n",
      "v102 44.8876409409\n",
      "*****************************\n",
      "v103 43.589541729\n",
      "*****************************\n",
      "v104 43.589541729\n",
      "*****************************\n",
      "v105 42.5626087945\n",
      "*****************************\n",
      "v106 43.5580514516\n",
      "*****************************\n",
      "v107 0.0026241897814\n",
      "*****************************\n",
      "v108 42.532867977\n",
      "*****************************\n",
      "v109 42.532867977\n",
      "*****************************\n",
      "v110 0.0\n",
      "*****************************\n",
      "v111 43.589541729\n",
      "*****************************\n",
      "v112 0.334146832166\n",
      "*****************************\n",
      "v113 48.3760638903\n",
      "*****************************\n",
      "v114 0.026241897814\n",
      "*****************************\n",
      "v115 43.6446497144\n",
      "*****************************\n",
      "v116 43.5930406487\n",
      "*****************************\n",
      "v117 42.532867977\n",
      "*****************************\n",
      "v118 43.5991637582\n",
      "*****************************\n",
      "v119 44.3313127072\n",
      "*****************************\n",
      "v120 43.5930406487\n",
      "*****************************\n",
      "v121 43.5965395684\n",
      "*****************************\n",
      "v122 43.6061615976\n",
      "*****************************\n",
      "v123 44.3295632473\n",
      "*****************************\n",
      "v124 42.5284943274\n",
      "*****************************\n",
      "v125 0.0673542043894\n",
      "*****************************\n",
      "v126 43.589541729\n",
      "*****************************\n",
      "v127 43.589541729\n",
      "*****************************\n",
      "v128 42.532867977\n",
      "*****************************\n",
      "v129 0.0\n",
      "*****************************\n",
      "v130 43.5991637582\n",
      "*****************************\n",
      "v131 43.6446497144\n",
      "*****************************\n"
     ]
    }
   ],
   "source": [
    "#there's quite a bit of missing data in the training set\n",
    "def checkmissing(x):\n",
    "    for a in x.columns:\n",
    "        num=x[a].notnull().value_counts()[True]\n",
    "        print a,(114321-num)*100/114321.0\n",
    "        print \"*****************************\"\n",
    "checkmissing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18211,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The v22 column has over 18000 categories. My poor computer can't handle that. \n",
    "train['v22'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    87021\n",
       "0    27300\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is imbalanced\n",
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dummy code the missing data\n",
    "mss_train=train[train.columns[2:]].isnull().astype('int')\n",
    "mss_train.columns = [a+'_na' for a in train.columns[2:]]\n",
    "\n",
    "# remove low occuring categories from v22 column. Set aside the target column\n",
    "target= train['target']\n",
    "v22 = train['v22']\n",
    "train.drop(['target','ID'],axis=1,inplace=True)\n",
    "goodnum=train['v22'].value_counts()[(train['v22'].value_counts()>150)]\n",
    "train['v22']=train['v22'].apply(lambda x : x if x in goodnum.index else 'JUNK')\n",
    "\n",
    "#get the list of columns that are categorical\n",
    "cate_col=[a for a in train.columns if train[a].dtypes=='object']\n",
    "fummy =[a for a in train.columns if a not in cate_col]\n",
    "nummy= train[fummy]\n",
    "catty = train[cate_col]\n",
    "\n",
    "\n",
    "#dummy code categorical columns and sort columns, \n",
    "# which will be important to make sure that the test data columns are aligned. \n",
    "catty=pd.get_dummies(catty)\n",
    "sort_cols = np.sort(catty.columns)\n",
    "catty=catty[sort_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#On the numerical columns, impute missing data for now with the median value.\n",
    "imp=Imputer(axis=1,verbose=1,strategy='median')\n",
    "tainy=imp.fit_transform(nummy)\n",
    "\n",
    "#scale numerical columns\n",
    "scaler = StandardScaler().fit(tainy)\n",
    "tainy =scaler.transform(tainy)\n",
    "trn=np.hstack((tainy,catty.values,mss_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3        A   B      C\n",
      "target                \n",
      "0        10   6  26607\n",
      "1       217  47  83977\n",
      "************************\n",
      "v22     AAPP  ABF  ABOF  ACWE  ADDF  ADMI  ADMP  AFOZ  AFYU  AGDF ...    TG  \\\n",
      "target                                                            ...         \n",
      "0         41   43    69    50    63    50    38    41    39   505 ...    45   \n",
      "1        123  123   135   140   149   106   134   111   120  1881 ...   121   \n",
      "\n",
      "v22     TVR  UAG  VVI  VZF  WNI  WRI  YEP   YGJ  YOD  \n",
      "target                                                \n",
      "0        60   31   60   73   44   41   61   524   77  \n",
      "1       122  138  151  223  200  166  163  1595  226  \n",
      "\n",
      "[2 rows x 44 columns]\n",
      "************************\n",
      "v24        A     B      C      D      E\n",
      "target                                 \n",
      "0       1015  1929   4183   6731  13442\n",
      "1       2774  6221  16689  19602  41735\n",
      "************************\n",
      "v30        A    B      C     D     E     F     G\n",
      "target                                          \n",
      "0        482   26   7257  1453   922   501  2430\n",
      "1       1831  179  24921  3772  2051  2088  6298\n",
      "************************\n",
      "v31         A      B     C\n",
      "target                    \n",
      "0       24655   1685   283\n",
      "1       63692  17262  3287\n",
      "************************\n",
      "v47      A   B      C     D     E     F     G  H      I     J\n",
      "target                                                       \n",
      "0        3   9   9339   646  1975  1148  1320  0  11862   998\n",
      "1       35  41  46086  2511  3326  3174  2626  1  27209  2012\n",
      "************************\n",
      "v52        A     B     C     D     E     F     G     H     I     J     K     L\n",
      "target                                                                        \n",
      "0       2084  2207  2291  2340  2274  2431  2308  1890  2495  2708  2090  2182\n",
      "1       6841  7178  7390  7267  7008  7375  7111  6433  7765  8395  6859  7396\n",
      "************************\n",
      "v56       A  AA  AB  AC  AE    AF    AG  AH   AI  AJ ...      P  Q    R  T  \\\n",
      "target                                               ...                     \n",
      "0         3   7   0   0   1   547   391   3   37   0 ...   2332  0  122  0   \n",
      "1       183  53   2  11   1  1808  1773  22  286   2 ...   2665  6  473  1   \n",
      "\n",
      "v56        U     V  W  X    Y    Z  \n",
      "target                              \n",
      "0        249   131  0  0  126   15  \n",
      "1       1426  1346  7  1  188  184  \n",
      "\n",
      "[2 rows x 122 columns]\n",
      "************************\n",
      "v66         A      B      C\n",
      "target                     \n",
      "0       17030   6525   3745\n",
      "1       53323  11739  21959\n",
      "************************\n",
      "v71     A      B     C  D      F  G   I  K  L\n",
      "target                                       \n",
      "0       1   6863  2598  0  17834  1   3  0  0\n",
      "1       0  23392  6349  1  57260  4  13  1  1\n",
      "************************\n",
      "v74      A      B    C\n",
      "target                \n",
      "0        7  27231   62\n",
      "1       38  86329  654\n",
      "************************\n",
      "v75      A      B   C      D\n",
      "target                      \n",
      "0        5   9457   9  17829\n",
      "1       13  29735  15  57258\n",
      "************************\n",
      "v79       A      B      C     D      E    F  G     H     I    J     K  L  \\\n",
      "target                                                                     \n",
      "0        51   2774  10716  1975   6090   44  3   584  1151  147  1146  0   \n",
      "1       366  23027  23845  3327  19167  527  3  1420  3410  786  3162  1   \n",
      "\n",
      "v79        M   N     O     P    Q   R  \n",
      "target                                 \n",
      "0       1320   4   376   496  414   9  \n",
      "1       2626  45  2955  1721  592  41  \n",
      "************************\n",
      "v91         A      B      C    D     E     F      G\n",
      "target                                             \n",
      "0        6455   5364   5467   43   546  3457   5968\n",
      "1       20624  17319  17690  187  2660  9961  18577\n",
      "************************\n",
      "v107       A      B      C      D      E     F    G\n",
      "target                                             \n",
      "0       3457   5364   5968   5467   6455   546   43\n",
      "1       9961  17319  18577  17690  20624  2660  187\n",
      "************************\n",
      "v110        A      B     C\n",
      "target                    \n",
      "0       17306   9339   655\n",
      "1       38382  46087  2552\n",
      "************************\n",
      "v112       A     B     C     D     E      F     G     H     I     J  ...   \\\n",
      "target                                                               ...    \n",
      "0       2334   626   488  1635  1109   5494   408  1333  2487   999  ...    \n",
      "1       7211  2062  1567  5692  3639  16177  1294  4318  7737  2968  ...    \n",
      "\n",
      "v112      M     N     O     P     Q     R    S     T     U     V  \n",
      "target                                                            \n",
      "0       282  2151   825  1138   557   964  192   920  1021   442  \n",
      "1       911  6935  2836  3537  1789  3206  641  3060  3782  1441  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "************************\n",
      "v113      A  AA   AB    AC   AD   AE    AF    AG   AH   AI ...     Q    R  \\\n",
      "target                                                     ...              \n",
      "0        98   0  405  1950   65  243  1152   655  214  109 ...   179   47   \n",
      "1       236   1  868  4006  200  437  2416  1057  583  204 ...   415  134   \n",
      "\n",
      "v113      S     T    U     V     W     X    Y    Z  \n",
      "target                                              \n",
      "0       257   278  176   625   468   554  490  126  \n",
      "1       357  1330  436  1048  1029  1081  792  327  \n",
      "\n",
      "[2 rows x 36 columns]\n",
      "************************\n",
      "v125       A   AA   AB    AC   AD   AE   AF   AG   AH   AI  ...     Q    R  \\\n",
      "target                                                      ...              \n",
      "0        353  122   44   494  109  192  233  234   84  232  ...   235  264   \n",
      "1       1175  420  145  1451  377  641  883  697  259  673  ...   831  953   \n",
      "\n",
      "v125      S    T    U     V    W    X    Y     Z  \n",
      "target                                            \n",
      "0       189  158  226   797  215  182  202   372  \n",
      "1       620  457  857  2437  673  685  678  1223  \n",
      "\n",
      "[2 rows x 90 columns]\n",
      "************************\n"
     ]
    }
   ],
   "source": [
    "def checkcate(x):\n",
    "    for a in cate_col:\n",
    "        print pd.crosstab(target,x[a],aggfunc='counts')\n",
    "        print \"************************\"\n",
    "checkcate(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts=pd.read_csv('sample_submission.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('v3', 3, 3),\n",
       " ('v22', 44, 41),\n",
       " ('v24', 5, 5),\n",
       " ('v30', 7, 7),\n",
       " ('v31', 3, 3),\n",
       " ('v47', 10, 9),\n",
       " ('v52', 12, 12),\n",
       " ('v56', 122, 116),\n",
       " ('v66', 3, 3),\n",
       " ('v71', 9, 9),\n",
       " ('v74', 3, 3),\n",
       " ('v75', 4, 4),\n",
       " ('v79', 18, 17),\n",
       " ('v91', 7, 7),\n",
       " ('v107', 7, 7),\n",
       " ('v110', 3, 3),\n",
       " ('v112', 22, 22),\n",
       " ('v113', 36, 36),\n",
       " ('v125', 90, 90)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the categories between the test and train data are not identical\n",
    "[(a,len(train[a].value_counts()),len(test[a].value_counts())) for a in cate_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing for the test set data\n",
    "def prepoc(t):\n",
    "    t['v22']=t['v22'].apply(lambda x : x if x in goodnum.index else 'JUNK')\n",
    "\n",
    "    nummy2= t[fummy]\n",
    "    catty2 = t[cate_col]\n",
    "    catty2=pd.get_dummies(catty2)   \n",
    "    \n",
    "    # there are some categories in test set data that don't exist in the training set\n",
    "    # and vice versa. We have to sort that out to make sure the number and order of columns\n",
    "    # in the test and train are identical. \n",
    "    for a in catty.columns:\n",
    "        if a not in catty2.columns:\n",
    "            catty2[str(a)] = 0\n",
    "    for a in catty2.columns:\n",
    "        if a not in catty.columns:\n",
    "            catty2.drop(a,axis=1,inplace=True)\n",
    "    sor_col = np.sort(catty2.columns)\n",
    "    catty2 = catty2[sor_col]\n",
    "    print \"woohoo\"\n",
    "    \n",
    "    mss_traint=t[train.columns[2:]].isnull().astype('int')\n",
    "    mss_traint.columns = [a+'_na' for a in train.columns[2:]]\n",
    "\n",
    "    tainy2=imp.fit_transform(nummy2)\n",
    "    scaler = StandardScaler().fit(tainy2)\n",
    "    tainy =scaler.transform(tainy2)\n",
    "    tst=np.hstack((tainy2,catty2.values,mss_traint.values))\n",
    "    return tst,catty2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woohoo\n"
     ]
    }
   ],
   "source": [
    "t,catty2=prepoc(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#make columns in the test and train data are aligned\n",
    "jk=[a for a in catty.columns]\n",
    "lk=[a for a in catty2.columns]\n",
    "zip(jk,lk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifiers\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight='auto', criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "0.772937605514\n",
      "0.889578590378\n",
      "0.727388263361\n",
      "********************************\n",
      "RandomForestClassifier(bootstrap=True, class_weight='auto', criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.774372162595\n",
      "0.885382775914\n",
      "0.723254306684\n",
      "********************************\n",
      "LogisticRegression(C=1.0, class_weight='auto', dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.64862098827\n",
      "0.893585537219\n",
      "0.734036770334\n",
      "********************************\n",
      "GaussianNB()\n",
      "0.290725238583\n",
      "0.852243474131\n",
      "0.528804488395\n",
      "********************************\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.770138469747\n",
      "0.891697321043\n",
      "0.727193952552\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='auto')\n",
    "#gbc = GradientBoostingClassifier(n_estimators=500)\n",
    "#abc = AdaBoostClassifier()\n",
    "etc = ExtraTreesClassifier(n_estimators=50,class_weight='auto')\n",
    "gnb = GaussianNB()\n",
    "svc = LinearSVC()\n",
    "rfc = RandomForestClassifier(n_estimators=50,class_weight='auto')\n",
    "bag =[etc,rfc,lr,gnb,svc]\n",
    "\n",
    "\n",
    "print \"Training classifiers\"\n",
    "for a in bag:\n",
    "    print a\n",
    "    print cross_val_score(a,trn,target).mean()\n",
    "    print cross_val_score(a,trn,target,scoring='average_precision').mean()\n",
    "    print cross_val_score(a,trn,target,scoring='roc_auc').mean()\n",
    "    a.fit(trn,target)\n",
    "    print \"********************************\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mediocre performance over all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try some good old dimensionality reduction\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "n_components = 50\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=5000)\n",
    "X_ipca = ipca.fit_transform(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=False, class_weight='balanced',\n",
      "           criterion='entropy', max_depth=None, max_features='auto',\n",
      "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "0.770392141426\n",
      "0.726402302665\n",
      "********************************\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='entropy', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.771214387558\n",
      "0.730302421633\n",
      "********************************\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.625099500529\n",
      "0.723846944359\n",
      "********************************\n",
      "GaussianNB()\n",
      "0.719045494704\n",
      "0.673210803859\n",
      "********************************\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "0.763700457484\n",
      "0.719376579052\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')\n",
    "#gbc = GradientBoostingClassifier(n_estimators=500)\n",
    "#abc = AdaBoostClassifier()\n",
    "etc = ExtraTreesClassifier(n_estimators=100,class_weight='balanced',criterion='entropy')\n",
    "gnb = GaussianNB()\n",
    "svc = LinearSVC()\n",
    "rfc = RandomForestClassifier(n_estimators=100,class_weight='balanced',criterion='entropy')\n",
    "bag =[etc,rfc,lr,gnb,svc]\n",
    "#test classifiers again\n",
    "for a in bag:\n",
    "    print a\n",
    "    print cross_val_score(a,X_ipca,target).mean()\n",
    "    print cross_val_score(a,X_ipca,target,scoring='roc_auc').mean()\n",
    "    print \"********************************\"\n",
    "\n",
    "#pca = PCA(n_components=n_components)\n",
    "#X_pca = pca.fit_transform(trn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Performance didn't change much with PCA. Naive bayes improved."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#make kaggle submissions file\n",
    "sub = etc.predict_proba(t)\n",
    "subpd=pd.DataFrame(sub)\n",
    "#subpd['PredictedProb'] = subpd[range(1,11,2)].sum(axis=1)/5\n",
    "subpd['PredictedProb'] = subpd[0]\n",
    "subpd['ID'] = test['ID']\n",
    "subpd[['ID','PredictedProb']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use random forests to pick out important features. Train classifiers based on \n",
    "# these columns\n",
    "feat_import=np.where(rfc.feature_importances_ >0.01)\n",
    "\n",
    "trn_ft=trn.T[feat_import].T\n",
    "for a in bag:\n",
    "    print cross_val_score(a,trn_ft,target).mean()\n",
    "    print cross_val_score(a,trn_ft,target,scoring='average_precision').mean()\n",
    "    print cross_val_score(a,trn_ft,target,scoring='roc_auc').mean()\n",
    "    print \"********************************\"\n",
    "\n",
    "rand_gp =[a.predict(trn_ft) for a in rand_ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do another training session but using imputation with mean values\n",
    "#On the numerical columns, impute missing data for now with the median value.\n",
    "imp=Imputer(axis=1,verbose=1)\n",
    "tainy=imp.fit_transform(nummy)\n",
    "\n",
    "#scale numerical columns\n",
    "scaler = StandardScaler().fit(tainy)\n",
    "tainy =scaler.transform(tainy)\n",
    "trn=np.hstack((tainy,catty.values,mss_train.values))\n",
    "\n",
    "print \"Training classifiers\"\n",
    "for a in bag:\n",
    "    print a\n",
    "    print cross_val_score(a,trn,target).mean()\n",
    "    print cross_val_score(a,trn,target,scoring='average_precision').mean()\n",
    "    print cross_val_score(a,trn,target,scoring='roc_auc').mean()\n",
    "    a.fit(trn,target)\n",
    "    print \"********************************\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# perhaps try stacking models with a mega classifer\n",
    "lr3 = LogisticRegression(class_weight='auto')\n",
    "gbc3 = GradientBoostingClassifier()\n",
    "abc3 = AdaBoostClassifier()\n",
    "etc3 = ExtraTreesClassifier(n_estimators=50,class_weight='auto')\n",
    "gnb3 = GaussianNB()\n",
    "svc3 = LinearSVC()\n",
    "rfc3 = RandomForestClassifier(n_estimators=100,class_weight='auto')\n",
    "bag3 =[gbc3,abc3,etc3,rfc3,lr3,gnb3,svc3]\n",
    "\n",
    "\n",
    "#make probability predictions on training data for each model\n",
    "# linearSVC does not do probablities.\n",
    "grp =[a.predict_proba(trn) for a in bag[:-1]]\n",
    "grp.append(bag[-1].predict(trn))  \n",
    "\n",
    "grppred=pd.DataFrame(grp[:-1])\n",
    "grppred[13] = grp[-1]\n",
    "\n",
    "#train stackers with classifiers in bag3\n",
    "print \"training stackers\"\n",
    "for a in bag3:\n",
    "    print a\n",
    "    a.fit(grppred,target)\n",
    "    print \"********************************\"\n",
    "\n",
    "#make predictions on test data\n",
    "print \"trying predictions\"\n",
    "grp_tst=[a.predict_proba(t) for a in bag[:-1]]\n",
    "grp_tst.append(bag[-1].predict(t))\n",
    "   \n",
    "grppred_tst=pd.DataFrame(np.hstack(grp_tst[:-1]))\n",
    "grppred_tst[13] = grp_tst[-1]\n",
    "\n",
    "sub=[a.predict_proba(grppred_tst) for a in bag3[:-1]]   \n",
    "\n",
    "#make submissions file\n",
    "subpd=pd.DataFrame(np.hstack(sub))\n",
    "#subpd['PredictedProb'] = subpd[range(1,11,2)].sum(axis=1)/5\n",
    "subpd['PredictedProb'] = subpd[1]\n",
    "subpd['ID'] = test['ID']\n",
    "subpd[['ID','PredictedProb']].to_csv('submission.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
